{
  "nbformat_minor": 0, 
  "nbformat": 4, 
  "cells": [
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "%matplotlib inline"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "\n# Hyperparameter Selection 1\n\n\nThis example demonstrates how to do model selection in a feature representation pipeline using a grid search\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "# Author: David Burns\n# License: BSD\n\nimport seglearn as sgl\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV, GroupKFold\nfrom sklearn.preprocessing import StandardScaler\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\n\ndef plot_grid_search(cv_results, grid_param_1, grid_param_2, name_param_1, name_param_2):\n    # plotting grid results from David Alvarez on Stack Overflow\n\n    # Get Test Scores Mean and std for each grid search\n    scores_mean = cv_results['mean_test_score']\n    scores_mean = np.array(scores_mean).reshape(len(grid_param_2),len(grid_param_1))\n    scores_sd = cv_results['std_test_score']\n    scores_sd = np.array(scores_sd).reshape(len(grid_param_2),len(grid_param_1))\n\n    # Plot Grid search scores\n    _, ax = plt.subplots(1,1)\n\n    # Param1 is the X-axis, Param 2 is represented as a different curve (color line)\n    for idx, val in enumerate(grid_param_2):\n        ax.plot(grid_param_1, scores_mean[idx,:], '-o', label= name_param_2 + ': ' + str(val))\n\n    ax.set_title(\"Grid Search Scores\", fontsize=20, fontweight='bold')\n    ax.set_xlabel(name_param_1, fontsize=16)\n    ax.set_ylabel('CV Average Score', fontsize=16)\n    ax.legend(loc=\"best\", fontsize=15)\n    ax.grid('on')\n\n\n# load the data\ndata = sgl.load_watch()\nX = sgl.make_ts_data(data['X'])\ny = data['y']\ng = data['subject']\n\n# use subject id to group folds\nsplitter = GroupKFold(n_splits=3)\ncv = splitter.split(X,y,groups = g)\n\n# create a feature representation pipeline\nest = Pipeline([('features', sgl.FeatureRep()),\n                ('scaler', StandardScaler()),\n                ('rf', RandomForestClassifier())])\n\npipe = sgl.SegPipe(est)\n\n\n# create a parameter dictionary using the SegPipe API - which is similar to the sklearn API\n#\n# parameters passed to an estimator in the ``feed`` pipeline are keyed ``f$estimator__parameter``\n# parameters passed to an estimator in the ``est`` pipeline are keyed ``e$estimator__parameter``\n#\n# when the ``feed`` or ``est`` pipeline is not a pipeline, but just a single estimator\n# the parameter would be keyed f$parameter or e$parameter respectively\n#\n# note that if you want to set a parameter to a single value, it will still need to be as a list\n\npar_grid = {'s$width' : [50,100,200],\n            's$overlap' : [0., 0.5],\n            'rf__n_estimators' : [20]}\n\nclf = GridSearchCV(pipe, par_grid, cv=cv)\nclf.fit(X, y)\n\nplot_grid_search(clf.cv_results_, par_grid['s$width'],\n                 par_grid['s$overlap'],'width', 'overlap')\nplt.show()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }
  ], 
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2", 
      "name": "python2", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "2.7.11", 
      "pygments_lexer": "ipython2", 
      "codemirror_mode": {
        "version": 2, 
        "name": "ipython"
      }
    }
  }
}
{
  "nbformat_minor": 0, 
  "nbformat": 4, 
  "cells": [
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "%matplotlib inline"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "\n# Scoring Time Series Estimators\n\n\nThis examples demonstrates some of the caveats / issues when trying to\ncalculate performance scores for time series estimators.\n\nThis pipeline has been designed to evaluate performance using\nsegments (not series') as instances of the data.\n\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "# Author: David Burns\n# License: BSD\n\n\nfrom seglearn.transform import FeatureRep, SegmentX\nfrom seglearn.pipe import SegPipe\nfrom seglearn.datasets import load_watch\nfrom seglearn.util import make_ts_data\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, cross_validate\nfrom sklearn.metrics import f1_score, confusion_matrix, make_scorer\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport itertools"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "CONFUSION PLOT\n#############################################\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "def plot_confusion_matrix(cm, classes,\n                          normalize=True,\n                          cmap=plt.cm.Blues):\n    ''' plots confusion matrix '''\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "SETUP\n#############################################\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "# load the data\ndata = load_watch()\nX = make_ts_data(data['X'])\ny = data['y']\n\n# create a feature representation pipeline\nest = Pipeline([('features', FeatureRep()),\n                ('scaler', StandardScaler()),\n                ('rf', RandomForestClassifier())])\npipe = SegPipe(est)\n\n# split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "OPTION 1: Use the score SegPipe score method\n#############################################\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "pipe.fit(X_train,y_train)\nscore = pipe.score(X_test, y_test)\nprint(\"Accuracy score: \", score)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "OPTION 2: generate true and predicted target values for the segments\n#####################################################################\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "y_true, y_pred = pipe.predict(X_test, y_test)\n# use any of the sklearn scorers\nf1_macro = f1_score(y_true, y_pred, average='macro')\nprint(\"F1 score: \", f1_macro)\n\ncm = confusion_matrix(y_true, y_pred)\nplot_confusion_matrix(cm, data['y_labels'])"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "OPTION 3: scoring during model selection\n#########################################\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "# model selection using the built-in score method for the final estimator\ncv_scores = cross_validate(pipe, X, y, cv = 4, return_train_score=True)\nprint(\"CV Scores: \", pd.DataFrame(cv_scores))\n\n# model selection with scoring functions / dictionaries\n#\n# unfortunately, this is not possible withing the current framework due to how\n# scoring is implemented within the model_selection functions / classes of sklearn\n# running the code below will cause an error, because the model_selection\n# functions / classes do not have access to y_true for the segments\n#\n# >>> scoring = ['accuracy','precision_macro','recall_macro','f1_macro']\n# >>> cv_scores = cross_validate(pipe, X, y, cv = 4, return_train_score=True, scoring=scoring)\n#\n# workarounds for this issue are outlined below"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "SCORING WORKAROUND 1: USE ANOTHER SCORER FUNCTION\n##################################################\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "# ``SegPipe`` can be initialized with a scorer callable made with sklearn.metrics.make_scorer\n# this can be used to cross_validate or grid search with any 1 score\n\nscorer = make_scorer(f1_score, average='macro')\npipe = SegPipe(est, scorer = scorer)\ncv_scores = cross_validate(pipe, X, y, cv = 4, return_train_score=True)\nprint(\"CV F1 Scores: \", pd.DataFrame(cv_scores))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "SCORING WORKAROUND 2: WORK OUTSIDE THE PIPELINE\n#################################################\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "# If you want to have multiple score computed, the only way is as follows\n#\n# First transform the time series data into segments and then score the ``est`` part of the\n# pipeline.\n#\n# The disadvantage of this is that the parameters of the ``seg`` pipeline cannot be\n# optimized with this approach\n\nsegmenter = SegmentX()\nX_seg, y_seg, _ = segmenter.fit_transform(X, y)\nscoring = ['accuracy','precision_macro','recall_macro','f1_macro']\ncv_scores = cross_validate(est, X_seg, y_seg,\n                           cv=4, return_train_score=False, scoring=scoring)\nprint(\"CV Scores (workaround): \", pd.DataFrame(cv_scores))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }
  ], 
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2", 
      "name": "python2", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "2.7.11", 
      "pygments_lexer": "ipython2", 
      "codemirror_mode": {
        "version": 2, 
        "name": "ipython"
      }
    }
  }
}